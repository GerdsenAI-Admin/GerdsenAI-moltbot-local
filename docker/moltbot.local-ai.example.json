{
  // Moltbot Local AI Configuration Example
  // Copy this file to ~/.clawdbot/moltbot.json and customize

  "models": {
    "mode": "merge",
    "providers": {
      // vLlama - Ollama model management + vLLM inference
      "vllama": {
        "baseUrl": "http://127.0.0.1:11435/v1",
        "apiKey": "vllama-local",
        "api": "openai-completions",
        "models": [
          {
            "id": "devstral",
            "name": "Devstral",
            "reasoning": false,
            "input": ["text"],
            "contextWindow": 128000,
            "maxTokens": 8192,
            "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 }
          }
        ]
      },

      // LM Studio - Cross-platform local inference
      "lmstudio": {
        "baseUrl": "http://127.0.0.1:1234/v1",
        "apiKey": "lmstudio",
        "api": "openai-responses",
        "models": [
          {
            "id": "qwen2.5-32b-instruct",
            "name": "Qwen 2.5 32B Instruct",
            "reasoning": false,
            "input": ["text"],
            "contextWindow": 128000,
            "maxTokens": 8192,
            "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 }
          }
        ]
      },

      // Ollama - Easy local model management
      "ollama": {
        "baseUrl": "http://127.0.0.1:11434/v1",
        "apiKey": "ollama",
        "api": "openai-completions",
        "models": [
          {
            "id": "mistral:7b-instruct",
            "name": "Mistral 7B Instruct",
            "reasoning": false,
            "input": ["text"],
            "contextWindow": 32768,
            "maxTokens": 4096,
            "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 }
          },
          {
            "id": "llama3.2:3b",
            "name": "Llama 3.2 3B",
            "reasoning": false,
            "input": ["text"],
            "contextWindow": 128000,
            "maxTokens": 4096,
            "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 }
          },
          {
            "id": "qwen2.5:7b-instruct",
            "name": "Qwen 2.5 7B Instruct",
            "reasoning": false,
            "input": ["text"],
            "contextWindow": 128000,
            "maxTokens": 8192,
            "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 }
          },
          {
            "id": "nemotron:8b",
            "name": "Nemotron 8B",
            "reasoning": true,
            "input": ["text"],
            "contextWindow": 128000,
            "maxTokens": 8192,
            "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 }
          }
        ]
      }
    }
  },

  // Enable local AI plugins
  "plugins": {
    "enabled": [
      "local-ai-discovery",
      "vectordb-chroma",
      "reranker",
      "document-ingest",
      "tool-validator"
    ],

    // Local AI Discovery - auto-detect models
    "local-ai-discovery": {
      "enabled": true,
      "backends": {
        "vllama": { "enabled": true },
        "lmstudio": { "enabled": true },
        "ollama": { "enabled": true }
      },
      "discoveryIntervalMs": 30000,
      "autoRegister": true
    },

    // Chroma Vector DB
    "vectordb-chroma": {
      "host": "http://localhost:8000",
      "collectionName": "moltbot_memory"
    },

    // Reranker for improved RAG
    "reranker": {
      "provider": "local",
      "local": {
        "baseUrl": "http://localhost:8080",
        "model": "BAAI/bge-reranker-base"
      },
      "topK": 10,
      "minScore": 0.0
    },

    // Document Ingestion
    "document-ingest": {
      "addToMemory": true,
      "includeMetadata": true,
      "supportedFormats": ["pdf", "docx", "txt", "md", "html"]
    },

    // Tool Validator for local models
    "tool-validator": {
      "enabled": true,
      "repairStrategy": "coerce",
      "blockDangerousCalls": true,
      "allowToolNameFuzzyMatch": true,
      "logValidationErrors": true
    }
  },

  // Default agent configuration
  "agents": {
    "defaults": {
      "model": {
        // Use Ollama by default (most compatible)
        "primary": "ollama/mistral:7b-instruct",
        // Fallback to smaller model if needed
        "fallback": "ollama/llama3.2:3b"
      },
      // Memory search with local embeddings
      "memorySearch": {
        "enabled": true,
        "provider": "local",
        "sources": ["memory", "sessions"]
      }
    }
  },

  // Gateway configuration
  "gateway": {
    "mode": "local",
    "bind": "loopback",
    "port": 18789
  }
}
